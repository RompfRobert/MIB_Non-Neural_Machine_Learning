{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:10_days_AI] *",
      "language": "python",
      "name": "conda-env-10_days_AI-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:05.659197Z",
          "start_time": "2020-10-27T16:27:03.972533Z"
        },
        "id": "PAK0c2qIqV3V"
      },
      "source": [
        "!pip install gradio > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:10.424473Z",
          "start_time": "2020-10-27T16:27:05.660766Z"
        },
        "id": "0ur8JFPlqV3Z",
        "outputId": "5dbf43a6-e5ed-48da-af87-f340463a6d02"
      },
      "source": [
        "! python -m spacy download en \n",
        "! pip install wordcloud > /dev/null\n",
        "! wget https://gitlab.com/andras.simonyi/10_days_AI_training_data/raw/master/sentiment.tsv?inline=false -O sentiment.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (2.3.1)\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.17.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
            "Requirement already satisfied: setuptools in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.6.0.post20200814)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.46.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.1 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.10)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages/en_core_web_sm\n",
            "-->\n",
            "/home/alatus/anaconda3/envs/10_days_AI/lib/python3.7/site-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "--2020-10-27 17:27:09--  https://gitlab.com/andras.simonyi/10_days_AI_training_data/raw/master/sentiment.tsv?inline=false\n",
            "Resolving gitlab.com (gitlab.com)... 172.65.251.78\n",
            "Connecting to gitlab.com (gitlab.com)|172.65.251.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘sentiment.tsv’\n",
            "\n",
            "sentiment.tsv           [ <=>                ] 437.05K  2.80MB/s    in 0.2s    \n",
            "\n",
            "2020-10-27 17:27:10 (2.80 MB/s) - ‘sentiment.tsv’ saved [447540]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m5Za6CvqV3c"
      },
      "source": [
        "# Task: sentiment classification\n",
        "\n",
        "The task is to classify one-sentence long movie reviews/opinions according to the sentiment they express. There are only two categories: positive and negative sentiment.\n",
        "\n",
        "\n",
        "> \"Data source: [UMICH SI650 - Sentiment Classification](https://www.kaggle.com/c/si650winter11/data)\n",
        "\n",
        "> Training data: 7086 lines. \n",
        "  \n",
        "> Format: 1|0 (tab) sentence\n",
        "\n",
        "> Test data: 33052 lines, each contains one sentence. \n",
        "\n",
        "> The data was originally collected from opinmind.com (which is no longer active).\"\n",
        "\n",
        "The data is in the file \"sentiment.tsv\".\n",
        "\n",
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:10.729070Z",
          "start_time": "2020-10-27T16:27:10.425698Z"
        },
        "id": "vrRNrr0PqV3e",
        "outputId": "c5050ecc-b9e4-4ab2-8adc-73be563291b2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('sentiment.tsv', sep='\\t', \n",
        "                 quoting=3, # Quotes are _never_ field separators\n",
        "                 header=None)\n",
        "\n",
        "df.head()\n",
        "\n",
        "df = df[[1,0]] # reorder columns\n",
        "\n",
        "df.rename(columns={1:\"text\", 0:\"sentiment\"}, inplace=True) # rename columns\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Da Vinci Code book is just awesome.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this was the first clive cussler i've ever rea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i liked the Da Vinci Code a lot.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i liked the Da Vinci Code a lot.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0            The Da Vinci Code book is just awesome.          1\n",
              "1  this was the first clive cussler i've ever rea...          1\n",
              "2                   i liked the Da Vinci Code a lot.          1\n",
              "3                   i liked the Da Vinci Code a lot.          1\n",
              "4  I liked the Da Vinci Code but it ultimatly did...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z8oEx3kqV3g"
      },
      "source": [
        "# Splitting into train, validation and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T13:46:49.883522Z",
          "start_time": "2020-10-27T13:46:49.878706Z"
        },
        "id": "uytmPMZJqV3h"
      },
      "source": [
        "Before doing anything else (!) we divide our data into train, validation and test parts,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:11.105130Z",
          "start_time": "2020-10-27T16:27:10.730198Z"
        },
        "id": "U0ft555qqV3i",
        "outputId": "f29e945c-1198-44e5-e022-85b0cb52bd51"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test_valid = train_test_split(df, test_size = 0.2, shuffle=True, \n",
        "                                           random_state=13) # fix the seed\n",
        "\n",
        "df_test, df_valid = train_test_split(df_test_valid, test_size = 0.5)\n",
        "\n",
        "print(len(df_train), len(df_valid), len(df_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5668 709 709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dWlQoB5qV3j"
      },
      "source": [
        "# Inspecting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:11.132382Z",
          "start_time": "2020-10-27T16:27:11.109013Z"
        },
        "id": "4vuFPFnPqV3k",
        "outputId": "279bb262-ce8d-4f91-b017-e0b518090196"
      },
      "source": [
        "df_train.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5668.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.559104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.496538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         sentiment\n",
              "count  5668.000000\n",
              "mean      0.559104\n",
              "std       0.496538\n",
              "min       0.000000\n",
              "25%       0.000000\n",
              "50%       1.000000\n",
              "75%       1.000000\n",
              "max       1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T13:47:24.072726Z",
          "start_time": "2020-10-27T13:47:24.069408Z"
        },
        "id": "YPW_Mm0FqV3l"
      },
      "source": [
        "We can examine the lengths of sentences as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:11.161877Z",
          "start_time": "2020-10-27T16:27:11.134596Z"
        },
        "id": "itaBKF-GqV3m",
        "outputId": "a51b2c2b-ea93-4a67-b60d-2d6408cad4d3"
      },
      "source": [
        "n_chars = df_train.text.apply(lambda x: len(x))\n",
        "\n",
        "n_chars.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    5668.000000\n",
              "mean       60.100565\n",
              "std        37.931478\n",
              "min        18.000000\n",
              "25%        32.000000\n",
              "50%        48.000000\n",
              "75%        77.000000\n",
              "max       203.000000\n",
              "Name: text, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T13:47:24.072726Z",
          "start_time": "2020-10-27T13:47:24.069408Z"
        },
        "id": "NN-v8ayEqV3n"
      },
      "source": [
        "The first sentence with the maximal length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:11.171622Z",
          "start_time": "2020-10-27T16:27:11.164197Z"
        },
        "id": "u_-LEgMFqV3n",
        "outputId": "3c9d7663-0582-46a5-d8ae-801d389bdb2b"
      },
      "source": [
        "long_sentence = df_train.loc[n_chars.idxmax(), \"text\"]\n",
        "long_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A mother in Georgia wants her local school board to take Harry Potter out of the schools and libraries because, in her opinion, reading Harry Potter leads to witchcraft, which according to her is evil...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2k0VcF_qV3o"
      },
      "source": [
        "# Bag of words (BoW) representation of the texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOWpZ1oKqV3o"
      },
      "source": [
        "We will represent each text as a (sparse) vector of lemma (word root) counts for frequent lemmas in the training data. \n",
        "\n",
        "For tokenization and lemmatization we use [spaCy](https://spacy.io/), an open source Python NLP library, which can produce a list of unique lemma ids from the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:12.092020Z",
          "start_time": "2020-10-27T16:27:11.174153Z"
        },
        "id": "vFUxnt_ZqV3o"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\"]) # We need only the tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loMoye8AqV3p"
      },
      "source": [
        "spaCy can produce spaCy Doc objects from texts that contain their linguistic analysis, among others lemmas and their unique spaCy string ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:12.108855Z",
          "start_time": "2020-10-27T16:27:12.093173Z"
        },
        "id": "P7KWsPsdqV3p",
        "outputId": "9d22ddad-2db5-4a3b-d707-2cc056fb9874"
      },
      "source": [
        "doc = nlp(long_sentence)\n",
        "type(doc)\n",
        "\n",
        "print([token.lemma_ for token in doc ]) # Lemmas\n",
        "\n",
        "print([token.lemma for token in doc]) # Corresponding unique ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'mother', 'in', 'Georgia', 'want', '-PRON-', 'local', 'school', 'board', 'to', 'take', 'Harry', 'Potter', 'out', 'of', 'the', 'school', 'and', 'library', 'because', ',', 'in', '-PRON-', 'opinion', ',', 'read', 'Harry', 'Potter', 'lead', 'to', 'witchcraft', ',', 'which', 'accord', 'to', '-PRON-', 'be', 'evil', '...']\n",
            "[11901859001352538922, 7963322251145911254, 3002984154512732771, 309210702643012516, 7597692042947428029, 561228191312463089, 16319852998319793599, 13293160603192985325, 14899812206273857344, 3791531372978436496, 6789454535283781228, 5164779919001708464, 2416965663249996073, 1696981056005371314, 886050111519832510, 7425985699627899538, 13293160603192985325, 2283656566040971221, 1785747669126016609, 16950148841647037698, 2593208677638477497, 3002984154512732771, 561228191312463089, 14536103007527724270, 2593208677638477497, 11792590063656742891, 5164779919001708464, 2416965663249996073, 82546335403996757, 3791531372978436496, 17905374590688478165, 2593208677638477497, 7063653163634019529, 701735504652304602, 3791531372978436496, 561228191312463089, 10382539506755952630, 15036397985088571056, 10875615029400813363]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtcf4eDCqV3q"
      },
      "source": [
        "Now we have to convert these lists into BoW vectors. We could \"roll our own\", but, fortunately, scikit-learn has a feature extractor doing exactly that, the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) so, for the sake of simplicity, we will use that along with spaCy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:12.135549Z",
          "start_time": "2020-10-27T16:27:12.112341Z"
        },
        "id": "ZH0UXT4IqV3q",
        "outputId": "a0b0bd28-1c88-4bb5-d4d1-dae78a1ad74b"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(analyzer=lambda s: [token.lemma for token in nlp(s)], # use spaCy for analysis\n",
        "                     min_df= 0.001) # Ignore lemmas with lower document frequency\n",
        "cv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer=<function <lambda> at 0x7ffb08547b00>, min_df=0.001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:12.152613Z",
          "start_time": "2020-10-27T16:27:12.138337Z"
        },
        "id": "PPTRokBNqV3q",
        "outputId": "131a5446-751f-41d6-c317-ba2b60b814ba"
      },
      "source": [
        "sents = [\"I hate this movie.\", \"The movie is the worst I've seen.\"]\n",
        "bows = cv.fit_transform(sents).toarray() # CountVectorizer produces a sparse matrix so we convert to ndarray\n",
        "bows"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 1, 0, 0, 0, 1, 0, 1],\n",
              "       [1, 0, 2, 0, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BpfXAu6qV3r"
      },
      "source": [
        "Using the CountVectorizer we convert the text columns of our train, validation and  test data into three sparse matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:23.850423Z",
          "start_time": "2020-10-27T16:27:12.158064Z"
        },
        "id": "c44DTpo9qV3r",
        "outputId": "bd126296-f4fd-4b99-db78-774dcb161511"
      },
      "source": [
        "bows_train = cv.fit_transform(df_train.text)\n",
        "bows_train.sort_indices() # comes from TF2.0 sparse implementation, obscure requirement\n",
        "bow_length = bows_train.shape[1]\n",
        "print(\"BoW length:\", bow_length)\n",
        "bows_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BoW length: 375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5668x375 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 63159 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:26.001734Z",
          "start_time": "2020-10-27T16:27:23.851751Z"
        },
        "id": "GUDWksXkqV3r",
        "outputId": "df00d0d6-445c-4ca8-83bc-4e84d893f7ce"
      },
      "source": [
        "print(bows_train[0,:])\n",
        "\n",
        "bows_valid = cv.transform(df_valid.text)\n",
        "bows_valid.sort_indices() # comes from TF2.0 sparse implementation, obscure requirement\n",
        "bows_test = cv.transform(df_test.text)\n",
        "bows_test.sort_indices() # comes from TF2.0 sparse implementation, obscure requirement"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 14)\t1\n",
            "  (0, 49)\t1\n",
            "  (0, 54)\t2\n",
            "  (0, 100)\t1\n",
            "  (0, 187)\t1\n",
            "  (0, 258)\t1\n",
            "  (0, 366)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtUGA58XqV3s"
      },
      "source": [
        "# The model\n",
        "\n",
        "We build a feed-forward neural network for our binary classification task, which will be trained with cross-entropy loss and minibatch SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:27:27.836193Z",
          "start_time": "2020-10-27T16:27:26.002639Z"
        },
        "id": "fRJsiMpDqV3t",
        "outputId": "e10e216e-be70-4e68-f7e6-5afd94c65da2"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Parameters\n",
        "############\n",
        "\n",
        "hidden_size = 100\n",
        "\n",
        "# Model\n",
        "#######\n",
        "\n",
        "inputs = Input(shape=(bow_length,))\n",
        "\n",
        "# Hidden layer\n",
        "\n",
        "hidden_output = Dense(hidden_size, activation='relu')(inputs)\n",
        "\n",
        "# Softmax \n",
        "\n",
        "predictions = Dense(2, activation='softmax')(hidden_output)\n",
        "\n",
        "\n",
        "# Full model\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "# Optimizer\n",
        "####################\n",
        "\n",
        "optimizer = SGD(lr=0.1)\n",
        " \n",
        "\n",
        "# Compilation and fitting \n",
        "#########################\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy', # we use this cross entropy variant as the input is not \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training\n",
        "\n",
        "model.fit(x=bows_train, \n",
        "          y=df_train.sentiment.values,\n",
        "          validation_data=(bows_valid, df_valid.sentiment.values),\n",
        "          epochs=10,\n",
        "          batch_size=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
            "Train on 5668 samples, validate on 709 samples\n",
            "Epoch 1/10\n",
            "5668/5668 [==============================] - 0s 28us/sample - loss: 0.5351 - accuracy: 0.8296 - val_loss: 0.3995 - val_accuracy: 0.9295\n",
            "Epoch 2/10\n",
            "5668/5668 [==============================] - 0s 14us/sample - loss: 0.3191 - accuracy: 0.9446 - val_loss: 0.2514 - val_accuracy: 0.9661\n",
            "Epoch 3/10\n",
            "5668/5668 [==============================] - 0s 16us/sample - loss: 0.2056 - accuracy: 0.9642 - val_loss: 0.1739 - val_accuracy: 0.9718\n",
            "Epoch 4/10\n",
            "5668/5668 [==============================] - 0s 16us/sample - loss: 0.1471 - accuracy: 0.9704 - val_loss: 0.1325 - val_accuracy: 0.9788\n",
            "Epoch 5/10\n",
            "5668/5668 [==============================] - 0s 10us/sample - loss: 0.1146 - accuracy: 0.9734 - val_loss: 0.1077 - val_accuracy: 0.9803\n",
            "Epoch 6/10\n",
            "5668/5668 [==============================] - 0s 14us/sample - loss: 0.0945 - accuracy: 0.9760 - val_loss: 0.0919 - val_accuracy: 0.9803\n",
            "Epoch 7/10\n",
            "5668/5668 [==============================] - 0s 17us/sample - loss: 0.0807 - accuracy: 0.9774 - val_loss: 0.0810 - val_accuracy: 0.9803\n",
            "Epoch 8/10\n",
            "5668/5668 [==============================] - 0s 12us/sample - loss: 0.0710 - accuracy: 0.9801 - val_loss: 0.0733 - val_accuracy: 0.9788\n",
            "Epoch 9/10\n",
            "5668/5668 [==============================] - 0s 12us/sample - loss: 0.0630 - accuracy: 0.9808 - val_loss: 0.0655 - val_accuracy: 0.9873\n",
            "Epoch 10/10\n",
            "5668/5668 [==============================] - 0s 14us/sample - loss: 0.0578 - accuracy: 0.9822 - val_loss: 0.0610 - val_accuracy: 0.9873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffaac7bcdd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:32:56.921084Z",
          "start_time": "2020-10-27T16:32:56.917847Z"
        },
        "id": "LLz7HskEqV3u"
      },
      "source": [
        "# Implement a function, that \n",
        "# assumes model and CountVectorizer to be available in the global namespace\n",
        "# receives a string input\n",
        "# processes it with CountVectorizer\n",
        "# takes the processed BOW representation\n",
        "# feeds it to the model for prediction\n",
        "# formats the prediction as a dict with the keys \"Negative\" and \"Positive\"\n",
        "# 0th prediction corresponds to Negative probability, 1st to Positive\n",
        "# cast the probabilities to simple Python floats, Numpy floats do NOT work.\n",
        "# return this dict\n",
        "def predict_sentiment(input_string):\n",
        "    global model\n",
        "    global cv\n",
        "    goodbad = [\"Negative\",\"Positive\"]\n",
        "    bow = cv.transform([input_string])\n",
        "    prob_pred = model.predict(bow[0])\n",
        "    return {\"Negative\":float(prob_pred[0,0]),\"Positive\":float(prob_pred[0,1])}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:34:12.118032Z",
          "start_time": "2020-10-27T16:34:12.107270Z"
        },
        "id": "DJGCJnZqqV3u",
        "outputId": "9c86c0e0-0335-45dd-f4a5-dfd211852f7e"
      },
      "source": [
        "# Test the function!\n",
        "predict_sentiment(\"This film was pretty amazing.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
            "[[0.29418302 0.705817  ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Negative': 0.29418301582336426, 'Positive': 0.7058169841766357}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFDba3kJyG7n"
      },
      "source": [
        "#Task1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T16:34:09.427347Z",
          "start_time": "2020-10-27T16:32:59.649445Z"
        },
        "id": "Hpe7MeU-qV3v"
      },
      "source": [
        "# Import Gradio, and build an interface\n",
        "# Input is a textbox, outputs are \"label\"-s, and interpretation is set to default.\n",
        "\n",
        "import gradio as gr\n",
        "iface = gr.Interface(\n",
        "  fn=predict_sentiment, inputs=gr.inputs.Textbox(default=\"This film was pretty amazing\"),\n",
        "  outputs=\"label\", interpretation=\"default\")\n",
        "\n",
        "# Launch the interface, possibly use debug=True to make your life easier!\n",
        "iface.launch()#debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0GdxSetqV3v"
      },
      "source": [
        "Additional tasks: \n",
        "\n",
        "- Use the model interpretation tool of Gradio to observe some counterexamples, that do not work well!\n",
        "- Set the flagging folder and use the flagging capability of Gradio to collect 15 badly behaving examples in a CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7audyk8yF1o"
      },
      "source": [
        "#Task2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZvUoc8OyKCW"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "urlretrieve(\"https://gr-models.s3-us-west-2.amazonaws.com/mnist-model.h5\", \"mnist-model.h5\")\n",
        "model = tf.keras.models.load_model(\"mnist-model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8Sm-NdLyM2R"
      },
      "source": [
        "def recognize_digit(image):\n",
        "    image = image.reshape(1, -1)\n",
        "    prediction = model.predict(image).tolist()[0]\n",
        "    return {str(i): prediction[i] for i in range(10)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvDsvNjb0IdW"
      },
      "source": [
        "im = gr.inputs.Image(shape=(28, 28), image_mode='L', invert_colors=False, source=\"canvas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpSuNAC10LM9"
      },
      "source": [
        "iface = gr.Interface(\n",
        "    recognize_digit, \n",
        "    im, \n",
        "    gradio.outputs.Label(num_top_classes=3),\n",
        "    live=True,\n",
        "    interpretation=\"default\",\n",
        "    capture_session=True,\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}