{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgPp--avm3GV"
      },
      "source": [
        "High dimensional data is more frequent than one might first think, e.g., even a low resolution  grey scale image from the famous [MNIST datase](https://en.wikipedia.org/wiki/MNIST_database):\n",
        "\n",
        "<a href=\"https://miro.medium.com/max/245/1*nlfLUgHUEj5vW7WVJpxY-g.png\"><img src=\"https://drive.google.com/uc?export=view&id=1-XrK7beC0beocLB_NufEmKVsBbCRko88\" width=250px></a>\n",
        "\n",
        "(Image source: [Image Classification in 10 Minutes with MNIST Dataset](https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d))\n",
        "\n",
        "has 784 dimensions, as it contains intensity information for each of its $28\\times 28 = 784$ pixels. Image processing is far from being the only area with high-dimensional data. For instance, the frequently used  \"bag of words\" representation of text documents in NLP uses a separate dimension for each word in the data set's vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgoQMMiFnXKQ"
      },
      "source": [
        "In this **practical session**, we try to:\n",
        "\n",
        "* create a multi perceptron model,\n",
        "* create an **LR** scheduler manually,\n",
        "* find the maximum of **LR** with LRFinder function,\n",
        "* and use the OneCycle **LR** model to get the best results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7mWWqL7pGwq"
      },
      "source": [
        "# Task1\n",
        "##Loading mnist hand written dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peNyie3p8J18"
      },
      "source": [
        "We try to classificate the mnist handwritten dataset with multi-layer perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H91tXiEwyfsX"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e_Veih1yZYI"
      },
      "source": [
        "#Load the dataset from keras API\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGA_ACwVz2Yp"
      },
      "source": [
        "# Flatten the images\n",
        "# You need to reshape the dataset from 28x28 2D array to 784 1D array. So the image_vector size equal the desired number of shape.\n",
        "# Becuse the first layer of perceptron needs 1D array of nodes.\n",
        "image_vector_size = ...\n",
        "x_train = x_train.reshape() # with image_vector_size\n",
        "x_test = x_test.reshape() # with image_vector_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfKYFnlepQxs"
      },
      "source": [
        "#Task2\n",
        "##Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBzXUKiYfBuL"
      },
      "source": [
        "In this case, create the multiperceptron model, and set the initial hyperparameters, and we try to predict the handwritten numbers from 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qohqmkXk21a"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BItEdYs-cVN8"
      },
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.1 # The learning rate\n",
        "momentum = 0.0 # Momentum\n",
        "\n",
        "# create a model\n",
        "def create_model():\n",
        "      model = Sequential()\n",
        "      # Input layer\n",
        "      model.add(Dense()) #add activation function, size of hidden layers, and input_shape\n",
        "      # Output layer\n",
        "      model.add(Dense()) #add activation function, and output shape\n",
        "\n",
        "      # Compile a model\n",
        "      model.compile(loss=... , optimizer=SGD(learning_rate, momentum), metrics=['accuracy']) #use crossentropy loss function\n",
        "      return model\n",
        "model = create_model()\n",
        "#fit the modelmodel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVhzlB57lyjZ"
      },
      "source": [
        "#Fitting the model\n",
        "#In this step we train our model with training dataset, and measure the loss, and accuracy on training and validation set too.\n",
        "\n",
        "batch_size = 128\n",
        "epoch = 15\n",
        "\n",
        "results = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs= epoch,\n",
        "    batch_size = batch_size,\n",
        "    validation_data = (...), #add the validation set\n",
        "    verbose = 1 # We need this, because Colab does not like if we print 1000 lines. So we disable keras status prints\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R3Zt4ubbZtt"
      },
      "source": [
        "results.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ulJmnmptQCU"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.figure(figsize = (12,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(results.history['accuracy'])\n",
        "plt.plot(results.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='down right')\n",
        "\n",
        "# summarize history for loss\n",
        "plt.subplot(122)\n",
        "plt.plot(results.history['loss'])\n",
        "plt.plot(results.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "\n",
        "max_loss = np.max(results.history['loss'])\n",
        "min_loss = np.min(results.history['loss'])\n",
        "print(\"Maximum Loss : {:.4f}\".format(max_loss))\n",
        "print(\"\")\n",
        "print(\"Minimum Loss : {:.4f}\".format(min_loss))\n",
        "print(\"\")\n",
        "print(\"Loss difference : {:.4f}\".format((max_loss - min_loss)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOciI7zwtFP2"
      },
      "source": [
        "#Task3\n",
        "##LR Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIoUzsT7gU0i"
      },
      "source": [
        "In this case, try to create an LR Scheduler manually with scheduler function and LearningRateScheduler callback from keras API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2049XuNtJPk"
      },
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we5xxW37tQMr"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  #create statements:\n",
        "  #if epoch less than 10 get back the initial lr, \n",
        "  #but if the number of epochs greater than 10 get back this equation: lr * tf.math.exp(-0.1)\n",
        "  if ... :\n",
        "    return ...\n",
        "  else:\n",
        "    return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRY3qyhhtpT-"
      },
      "source": [
        "callback = tf.keras.callbacks.LearningRateScheduler(...) #use the scheduler function for the LR scheduler\n",
        "\n",
        "history = model.fit(\n",
        "          x_train, y_train,\n",
        "          epochs= 15,\n",
        "          batch_size = 100,\n",
        "          validation_data = (...), #use the validation set\n",
        "          verbose = 1, # We need this, because Colab does not like if we print 1000 lines. So we disable keras status prints\n",
        "          callbacks = ... #add the callback which is the LR Scheduler\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYVSY03juKS4"
      },
      "source": [
        "scheduled_lr = round(..., 5) #save the model.optimizer.lr like numpy array and round it with 5 points and print it\n",
        "print(scheduled_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrkJ2qxR13BD"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.figure(figsize = (12,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='down right')\n",
        "\n",
        "# summarize history for loss\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "\n",
        "max_loss = np.max(history.history['loss'])\n",
        "min_loss = np.min(history.history['loss'])\n",
        "print(\"Maximum Loss : {:.4f}\".format(max_loss))\n",
        "print(\"\")\n",
        "print(\"Minimum Loss : {:.4f}\".format(min_loss))\n",
        "print(\"\")\n",
        "print(\"Loss difference : {:.4f}\".format((max_loss - min_loss)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-koysvictHr"
      },
      "source": [
        "#Task4\n",
        "##LRFinder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x27O2FMYhEHc"
      },
      "source": [
        "There is a predefined learning rate finder function on [github](https://github.com/titu1994/keras-one-cycle/blob/master/clr.py) which we try. Please download the functions and try to use it with our hints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR9kUVJ5h2im"
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/solalatus/IBS_GF_kepzes/main/Big_Data_and_ML/04Hyperparameters/clr.py?token=AHL2UDKITYK3K27TYW4F7XLBDIE2M\" -O clr.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sL8tAI2cwGV"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnpk-PCsh93K"
      },
      "source": [
        "In this step we import the LRFinder function, and try to use it. This function need start and end number of learning rate. After that this function iterate all from start to end lr step by step, and measure the accuracies and losses on training and validation set too. After that, the function get back the maximum of lr."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i73iK_Hc7Os"
      },
      "source": [
        "from clr import LRFinder\n",
        "\n",
        "num_samples = #number of samples\n",
        "batch_size = #training batch size\n",
        "minimum_lr = #starting lr eg.: 1e-5\n",
        "maximum_lr = #maximum lr eg.: 10\n",
        "\n",
        "lr_callback = LRFinder(num_samples, batch_size,\n",
        "                       minimum_lr, maximum_lr,\n",
        "                       validation_data=(x_test, y_test),\n",
        "                       lr_scale='exp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP0fkH-0dXfO"
      },
      "source": [
        "# Ensure that number of epochs = 1 when calling fit()\n",
        "model.fit(x_train, y_train, epochs=1, batch_size=batch_size, callbacks=[lr_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzikoocIdvcU"
      },
      "source": [
        "#plot the lr vs loss and find the best of LR\n",
        "lr_callback.plot_schedule()\n",
        "max_lr = ... # from history of model!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkziq5V9pXMN"
      },
      "source": [
        "#Task5\n",
        "##One Cycle LR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT1F8canjnnJ"
      },
      "source": [
        "After the LRFinding we try to use One Cycle LR which is a predefenied function in this case from [github](https://github.com/titu1994/keras-one-cycle/blob/master/clr.py). In this step we use the previous result (maximum lr) like maximum learning rate in OneCycleLR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTPethDPplW2"
      },
      "source": [
        "from clr import OneCycleLR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrI-Uu8K3y6d"
      },
      "source": [
        "num_samples = ... # samples equal 10k\n",
        "batch_size = ... # like in the base model\n",
        "\n",
        "lr_manager = OneCycleLR(num_samples=...,\n",
        "                        batch_size=...,\n",
        "                        max_lr=...,\n",
        "                        end_percentage=...,) #add 0.1 to end_percentage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdEdeDI435qM"
      },
      "source": [
        "#use the lr_manager like callbacks\n",
        "#add the train set again and fit the new model\n",
        "model.fit(..., ..., epochs=1, batch_size=... , ...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fqF2nEF4cXD"
      },
      "source": [
        "#Plot the learning rate and momentum of one cycle LR\n",
        "\n",
        "print(\"LR Range : \", min(lr_manager.history['lr']), max(lr_manager.history['lr']))\n",
        "print(\"Momentum Range : \", min(lr_manager.history['momentum']), max(lr_manager.history['momentum']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(...) # learning rate\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Momentum')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(...) # momentum\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}