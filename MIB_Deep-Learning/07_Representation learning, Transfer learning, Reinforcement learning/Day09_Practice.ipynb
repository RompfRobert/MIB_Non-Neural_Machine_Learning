{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xg6jPkX7sfzY"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment classification with the IMDB movie reviews dataset\n",
        "\n",
        "Goal is to test the effects of pretrained word embeddings, namely [word2vec](https://en.wikipedia.org/wiki/Word2vec), which is available in pre-trained form in [Gensim](https://radimrehurek.com/gensim/) from [here](https://github.com/RaRe-Technologies/gensim-data).\n",
        "\n",
        "We will train an lstm based sentiment classifier for the IMDB dataset first without and then with pre-trained embeddings to test the effect of transfer learning.\n",
        "\n",
        "## The IMDB dataset\n",
        "\n",
        "\"Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
        "\n",
        "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\"\n",
        "\n",
        "Source [here](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification).\n",
        "\n",
        "\n",
        "##  The techniques\n",
        "\n",
        "For this we will utilize [Keras Sequential API](https://keras.io/getting-started/sequential-model-guide/).\n",
        "\n",
        "This task's original is: [Sentiment detection with Keras, word embeddings and LSTM deep learning networks](https://www.liip.ch/en/blog/sentiment-detection-with-keras-word-embeddings-and-lstm-deep-learning-networks) and [here](https://github.com/plotti/keras_sentiment/blob/master/Imdb%20Sentiment.ipynb)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "j0JOfia9sfza"
      },
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ]
    },
    {
      "metadata": {
        "id": "C67k0_Zqsfzb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "412e64bf-75c8-4bd4-8cfa-5d60f1fc9492"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.datasets import imdb \n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense \n",
        "from keras.layers import LSTM \n",
        "from keras.layers.embeddings import Embedding \n",
        "from keras.preprocessing import sequence \n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BpuDsjqEsfzg"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "BATCH_SIZE = 500#64\n",
        "max_review_length = 500 #For truncting the maximum length of reviews in tokens\n",
        "optimizer = \"adam\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kQWsjwHQsfzl"
      },
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ]
    },
    {
      "metadata": {
        "id": "KE83mIwzsfzn"
      },
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility \n",
        "np.random.seed(7) \n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest \n",
        "top_words = 5000  #TODO!!!!\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aq5DXC8psfzr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aa989a17-f376-469b-aca1-5e87e13e43fe"
      },
      "cell_type": "code",
      "source": [
        "INDEX_FROM=3   # word index offset\n",
        "word_to_id = keras.datasets.imdb.get_word_index()\n",
        "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in X_train[0] ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EfFwF5H0sfzw"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "metadata": {
        "id": "wWMFcpKSsfzx"
      },
      "cell_type": "code",
      "source": [
        "# truncate and pad the review sequences \n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length) \n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwyQ60Jnsfz2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "aa67a2d8-edfe-4e20-bde6-b3b05f0b9a6a"
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X_train).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>490</th>\n",
              "      <th>491</th>\n",
              "      <th>492</th>\n",
              "      <th>493</th>\n",
              "      <th>494</th>\n",
              "      <th>495</th>\n",
              "      <th>496</th>\n",
              "      <th>497</th>\n",
              "      <th>498</th>\n",
              "      <th>499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>4472</td>\n",
              "      <td>113</td>\n",
              "      <td>103</td>\n",
              "      <td>32</td>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>178</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>52</td>\n",
              "      <td>154</td>\n",
              "      <td>462</td>\n",
              "      <td>33</td>\n",
              "      <td>89</td>\n",
              "      <td>78</td>\n",
              "      <td>285</td>\n",
              "      <td>16</td>\n",
              "      <td>145</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>106</td>\n",
              "      <td>607</td>\n",
              "      <td>624</td>\n",
              "      <td>35</td>\n",
              "      <td>534</td>\n",
              "      <td>6</td>\n",
              "      <td>227</td>\n",
              "      <td>7</td>\n",
              "      <td>129</td>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>687</td>\n",
              "      <td>23</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3693</td>\n",
              "      <td>42</td>\n",
              "      <td>38</td>\n",
              "      <td>39</td>\n",
              "      <td>...</td>\n",
              "      <td>26</td>\n",
              "      <td>49</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>566</td>\n",
              "      <td>30</td>\n",
              "      <td>579</td>\n",
              "      <td>21</td>\n",
              "      <td>64</td>\n",
              "      <td>2574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>19</td>\n",
              "      <td>14</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>226</td>\n",
              "      <td>251</td>\n",
              "      <td>7</td>\n",
              "      <td>61</td>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1    2    3    4    5     6    7    8    9    ...    490  491  492  \\\n",
              "0    0    0    0    0    0    0     0    0    0    0  ...   4472  113  103   \n",
              "1    0    0    0    0    0    0     0    0    0    0  ...     52  154  462   \n",
              "2    0    0    0    0    0    0     0    0    0    0  ...    106  607  624   \n",
              "3  687   23    4    2    2    6  3693   42   38   39  ...     26   49    2   \n",
              "4    0    0    0    0    0    0     0    0    0    0  ...     19   14    5   \n",
              "\n",
              "   493  494  495  496  497  498   499  \n",
              "0   32   15   16    2   19  178    32  \n",
              "1   33   89   78  285   16  145    95  \n",
              "2   35  534    6  227    7  129   113  \n",
              "3   15  566   30  579   21   64  2574  \n",
              "4    2    6  226  251    7   61   113  \n",
              "\n",
              "[5 rows x 500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "xGZ_IEYbsfz7"
      },
      "cell_type": "markdown",
      "source": [
        "## Create model"
      ]
    },
    {
      "metadata": {
        "id": "YnFfZCP9sfz9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "944ba03e-fdcb-471e-e37d-08851e189ab4"
      },
      "cell_type": "code",
      "source": [
        "# create the baseline model \n",
        "embedding_vector_length = 300 #fixed for being fair, word2vec has 300 later on\n",
        "model = Sequential() \n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length)) \n",
        "model.add(LSTM(100)) \n",
        "\n",
        "model.add(Dense(1, activation='sigmoid')) \n",
        "model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=['accuracy']) \n",
        "print(model.summary()) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 300)          1500000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,660,501\n",
            "Trainable params: 1,660,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZRf_FDWWsf0B"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ]
    },
    {
      "metadata": {
        "id": "W4D6Rmcpsf0B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e59c92ae-cd4d-44e4-878c-cb9eb8d8174d"
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/5\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4952 - acc: 0.7473 - val_loss: 0.3353 - val_acc: 0.8641\n",
            "Epoch 2/5\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.2800 - acc: 0.8918 - val_loss: 0.3157 - val_acc: 0.8692\n",
            "Epoch 3/5\n",
            "25000/25000 [==============================] - 83s 3ms/step - loss: 0.2262 - acc: 0.9127 - val_loss: 0.3137 - val_acc: 0.8760\n",
            "Epoch 4/5\n",
            "25000/25000 [==============================] - 84s 3ms/step - loss: 0.1883 - acc: 0.9304 - val_loss: 0.3401 - val_acc: 0.8741\n",
            "Epoch 5/5\n",
            "25000/25000 [==============================] - 84s 3ms/step - loss: 0.1635 - acc: 0.9402 - val_loss: 0.3324 - val_acc: 0.8606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0bf3e80a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "g5fgufbksf0G"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate model"
      ]
    },
    {
      "metadata": {
        "id": "oEBT4bCbsf0G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5ac5bf7-4435-437c-b2bb-f3133680167c"
      },
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model \n",
        "scores = model.evaluate(X_test, y_test, verbose=0) \n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 86.06%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ixjmd1eHsf0I"
      },
      "cell_type": "markdown",
      "source": [
        "## Predict something"
      ]
    },
    {
      "metadata": {
        "id": "8ibAiks1sf0J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c8501364-85b2-4dcc-d25f-b9ea1cbbca24"
      },
      "cell_type": "code",
      "source": [
        "bad = \"this movie was terrible and bad\"\n",
        "good = \"i really liked the movie and had fun\"\n",
        "for review in [good,bad]:\n",
        "    tmp = []\n",
        "    for word in review.split(\" \"):\n",
        "        tmp.append(word_to_id[word])\n",
        "    tmp_padded = sequence.pad_sequences([tmp], maxlen=max_review_length) \n",
        "    print(\"%s . Sentiment: %s\" % (review,model.predict(np.array([tmp_padded][0]))[0][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i really liked the movie and had fun . Sentiment: 0.8632016\n",
            "this movie was terrible and bad . Sentiment: 0.05640324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cYqZi0fZsf0L"
      },
      "cell_type": "markdown",
      "source": [
        "## Load word2vec embeddings"
      ]
    },
    {
      "metadata": {
        "id": "TUpyTB-hsf0L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9f2dc044-b648-4d15-a8e9-d594704ff93f"
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "w2v_matrix = np.zeros((len(id_to_word), embedding_vector_length))\n",
        "\n",
        "for wid, word in id_to_word.items():\n",
        "    try:\n",
        "      w2v_matrix[wid]=w2v_model[word]\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.7.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.7)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.7 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.7->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.7->boto3->smart-open>=1.2.1->gensim) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OkAh_bXpsf0N"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare pretrained embedding layer"
      ]
    },
    {
      "metadata": {
        "id": "yq8hu8tosf0O"
      },
      "cell_type": "code",
      "source": [
        "from keras.initializers import Constant\n",
        "\n",
        "word2vec_embeddings_layer = Embedding(len(id_to_word),\n",
        "                            embedding_vector_length,\n",
        "                            embeddings_initializer=Constant(w2v_matrix),\n",
        "                            input_length=max_review_length,\n",
        "                            trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Z_Dhg1Qsf0S"
      },
      "cell_type": "markdown",
      "source": [
        "## Build model with pretrained embedding"
      ]
    },
    {
      "metadata": {
        "id": "9Nhm9r4gsf0T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "b3514298-8af0-4d8f-8e65-3b9d940cdf5f"
      },
      "cell_type": "code",
      "source": [
        "# create the model using the pretrained embeddings\n",
        "model_pretrained = Sequential() \n",
        "\n",
        "model_pretrained.add(word2vec_embeddings_layer) \n",
        "\n",
        "model_pretrained.add(LSTM(100)) \n",
        " \n",
        "model_pretrained.add(Dense(1, activation='sigmoid')) \n",
        "model_pretrained.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=['accuracy']) \n",
        "print(model_pretrained.summary()) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 500, 300)          26576100  \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 26,736,601\n",
            "Trainable params: 26,736,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z8MU8JDRsf0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "bd9f389f-04ea-4645-c8e5-c915d9f9df6e"
      },
      "cell_type": "code",
      "source": [
        "model_pretrained.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/5\n",
            "25000/25000 [==============================] - 92s 4ms/step - loss: 0.5188 - acc: 0.7481 - val_loss: 0.3754 - val_acc: 0.8432\n",
            "Epoch 2/5\n",
            "25000/25000 [==============================] - 88s 4ms/step - loss: 0.3054 - acc: 0.8743 - val_loss: 0.3237 - val_acc: 0.8679\n",
            "Epoch 3/5\n",
            "25000/25000 [==============================] - 88s 4ms/step - loss: 0.2698 - acc: 0.8963 - val_loss: 0.3762 - val_acc: 0.8257\n",
            "Epoch 4/5\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.2910 - acc: 0.8839 - val_loss: 0.3101 - val_acc: 0.8746\n",
            "Epoch 5/5\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.2129 - acc: 0.9192 - val_loss: 0.3031 - val_acc: 0.8788\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0b8eb42e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "Az3kzI7fsf0Y"
      },
      "cell_type": "markdown",
      "source": [
        "## Test final score"
      ]
    },
    {
      "metadata": {
        "id": "WQvvI7Vmsf0Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "805667d0-b622-4a79-b996-9e54159dab34"
      },
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model \n",
        "scores = model_pretrained.evaluate(X_test, y_test, verbose=0) \n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 87.88%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}