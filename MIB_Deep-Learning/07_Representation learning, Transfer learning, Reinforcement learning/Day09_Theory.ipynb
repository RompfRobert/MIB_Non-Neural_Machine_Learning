{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhtwOJPKSJbm"
      },
      "source": [
        "<a id=\"representation\"></a>\n",
        "# Representation learning revisited\n",
        "\n",
        "After gaining insight about the architectures and procedures that make training of deep neural networks possible, let's get back to the topic of representations. Originally we motivated the usage of deep learning with two things: \n",
        "1. Superior performance\n",
        "\n",
        "That is the ability to surpass traditional models in relevant measures.\n",
        "\n",
        "2. End-to-end learning\n",
        "\n",
        "The ability to forego much of the manual feature engineering process necessary for classical models.\n",
        "\n",
        "The representation ability of deep learning models is in strong connection with the latter. \n",
        "Let us examine this in detail!\n",
        "\n",
        "## Learned representations in \"shallow\" networks: word2vec\n",
        "**\"Don't count, predict!\"**\n",
        "\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1uu00eAJi3-3tH2Iz8IMP9vz3iWtgdmqM\"><img src=\"https://drive.google.com/uc?export=view&id=1XVg5kBnHm3N5NrnRvTgwKf6Pk_0-TM7u\" width=600 heigth=600></a>\n",
        "\n",
        "\n",
        "With the publication of \"Distributed Representations of Words and Phrases and their Compositionality\" by [Mikolov et al. 2013](https://arxiv.org/pdf/1310.4546.pdf) a _huge_ shift occured in the NLP community, that led away from frequency based methods and introdiced the usage of prediction based methods for the generation of efficient language models. (First at word level.)\n",
        "\n",
        "### Schematics\n",
        "\n",
        "<a href=\"https://raw.githubusercontent.com/rohan-varma/paper-analysis/master/word2vec-papers/models.png\"><img src=\"https://drive.google.com/uc?export=view&id=1ZUwSGzEj4EWHCEabrrvmXadfRsdRF7_f\" width=600 heigth=600></a>\n",
        "\n",
        "<a href=\"https://i.stack.imgur.com/igSuE.png\"><img src=\"https://drive.google.com/uc?export=view&id=1R5hI-k4Zu8P6bE3oGgmJ5cMpDYshU7ML\" width=300 heigth=300></a>\n",
        "\n",
        "(Important to note that the invention of \"hierarchic softmax\" came from this research, since the many $v$ vocabulary width layers were consuming extreme amout of computation (by 2013 standards) for a vocabulary of 300k. Based on this there are CPU programmable efficient implementations of word2vec \"out of the box\", like in [Gensim](https://radimrehurek.com/gensim/models/word2vec.html).  \n",
        "\n",
        "### Advantage\n",
        "\n",
        "The real advantage of these \"word embeddings\" (which became the workhorse of NLP eversince) was not that they were useful in predicting the next words \"autocomplete style\" (as we have seen before in our training), but much more as general dense vector representations, \"embeddings\" of words. The main breakthrough of Mikolov et al. was to discover the deep structure that the vectorspaces exhibit after training!\n",
        "\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1heogQhMfvtiOSfPtKvmc2OtyGadAsOmd\"><img src=\"https://drive.google.com/uc?export=view&id=1HbQDy8orwiRH7SiaJjE5Gu5g99iavZa5\" width=600 heigth=600></a>\n",
        "\n",
        "\n",
        "A good analysis of this topic can be found in [Marek Rei's blogpost](http://www.marekrei.com/blog/dont-count-predict/).\n",
        "\n",
        "(Naturally, progress did not stop here, you can read up on successive generations of vector embedding for NLP [here](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a).)\n",
        "\n",
        "### Why can this be?\n",
        "\n",
        "For the model to solve the prediction task effectively, it has to come up with a representation that captures the salient features of the data in the most copact way, that is, it lossfully memorizes and compresses the data, during which it captures it's main features.\n",
        "\n",
        "**It turns out that the decisive advantage of deep learning based methods is exactly this: the \"byproduct\" of learning hierarchic, meaningful features during training.** Throughout this class, we will examine the effects and possibilities arising from this. \n",
        "\n",
        "Remember:\n",
        "**Representation is everything!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVpZvJHqSJbn"
      },
      "source": [
        "## New distance-metric oriented supervised tasks and loss functions\n",
        "\n",
        "The growing recognition that models trained on particular supervised tasks can learn representations useful for a wide range of different purposes led to a research into tasks and objectives that are more conducive to learning good distributed representations.\n",
        "\n",
        "One of the explicit goals of these objectives is to learn useful similarity distance metrics over the domain.\n",
        "\n",
        "Image similarity recognition (in particular face recognition) was the most important problem that motivated this research, but in the last few years the methods have been heavily used in other domains as well, e.g. for voice verification/identification. The most important characteristics of these tasks and objectives is that they\n",
        "\n",
        "- work with coarse grained ranking data about similarity, crucially, they can be used on labeled/classification data sets where the only available similarity information comes from class membership (examples belong to the same or different class)\n",
        "- produce (similarly to Word2Vec) _sparse distributed representations_ / _embeddings_ of the input \n",
        "- try to measure and maximize the quality of the distance-based similarity metric provided by the learned representations\n",
        "- the objective is typically to keep members of the same class close to each other and distant from those of other classes (in the embedding space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoM-fZXMSJbn"
      },
      "source": [
        "### Contrastive loss (2005)\n",
        "\n",
        "The first step in this direction was the introduction of __contrastive loss__ by Hadsell et al. in 2005 ([Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)), which is a loss function for __pairs of examples__ $\\langle \\mathbf x_1, \\mathbf x_2\\rangle$:\n",
        "\n",
        "$$\\mathcal L_{\\mathrm{Contrastive}} = (1-y)\\frac{1}{2}D(\\mathbf x_1, \\mathbf x_2)^2 + y \\frac{1}{2}(\\max(0, m-D(\\mathbf x_1, \\mathbf x_2))^2$$\n",
        "\n",
        "where $y$ is 0 if the two examples are similar and 1 if not, and D is a distance metric between the embeddings, e.g.,  Euclidean or cosine distance. The intention is obvious: bring similar examples closer and separate dissimilar examples by an $m$ margin.  \n",
        "\n",
        "The architecture used with this kind of loss is typically a so-called \"Siamese network\", in which embeddings are produced with the same network topology and weights for both examples:\n",
        "\n",
        "<a href=\"https://miro.medium.com/max/841/1*fUY-bpGFoUMWBkh_rychCQ.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1GnyGX-FsC_29NLGYEyJRNZYVi6kPabpj\" width=\"350px\"></a>\n",
        "\n",
        "(Image source: [Siamese Networks for Visual Tracking](https://medium.com/intel-student-ambassadors/siamese-networks-for-visual-tracking-96262eaaba77))\n",
        "\n",
        "__Challenges__\n",
        "\n",
        "Perhaps the most important challenge for contrastive loss is that incentives the model to keep examples from the same class very close to each other, so it is prone to concentrate the classes into small areas of the space. As a consequence, the learned representations lack distinctions within the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksNyh-fFSJbn"
      },
      "source": [
        "###  Triplet loss (2010)\n",
        "\n",
        "In many respects a follow-up to contastive loss, triplet loss tries to solve the class concentration problem of contrastive loss by considering data point _triplets_ and enforcing only the correct _relative distances_. The inputs are $\\langle A, P, N\\rangle$ triplets containing a so called $A$ _anchor example_, and a $P$ positive and an $N$ negative example, of which the former belongs to the same class as the anchor, while the second to a different one.\n",
        "\n",
        "The objective is to keep an (SVM-like) positive margin between the distance/similarity of the anchor and the positive and the anchor and the negative example:\n",
        "\n",
        "$$\n",
        "\\mathcal L(A, P, N) = \\max(D(A, P) - D(A, N) + margin, 0)\n",
        "$$\n",
        "\n",
        "Where the $D(\\cdot)$ distance function is commonly Euclidean or cosine distance between the embeddings of the examples.\n",
        "\n",
        "\n",
        "<a href=\"https://omoindrot.github.io/assets/triplet_loss/triplet_loss.png\"><img src=\"https://drive.google.com/uc?export=view&id=1w7Yx2yJbTJ7pdaLaUfOtWDNi9BRQLDiz\" width=\"500px\"></a>\n",
        "\n",
        "([Image source](https://omoindrot.github.io/triplet-loss))\n",
        "\n",
        "__Challenges__\n",
        "\n",
        "Training with triplet loss is known to be challenging, since the results are dependent on the \"difficulty\" of the triplets that are fed into to network. As training on all possible triplets is usually unfeasible and would not be desirable anyway, smart and often resource intensive \"triplet mining\" techniques have to employed to select the \"hard\" and \"semi-hard\" triplets in which the negative example is closer to the anchor than the positive or the distance is smaller than the margin:\n",
        "\n",
        "<a href=\"https://omoindrot.github.io/assets/triplet_loss/triplets.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Ur_mBDuhDRdSacfarQ9CnhGGCstUJadX\" width=\"400px\"></a>\n",
        "\n",
        "([Image source](https://omoindrot.github.io/triplet-loss))\n",
        "\n",
        "__See also__\n",
        "\n",
        "+ A good, TF-based introduction from which the above figures were taken: [Triplet Loss and Online Triplet Mining in TensorFlow](https://omoindrot.github.io/triplet-loss)\n",
        "+ The classic, original triplet loss paper on image similarity detection: [Chechik et al.: Large Scale Online Learning of Image Similarity Through Ranking (2010)](http://www.jmlr.org/papers/volume11/chechik10a/chechik10a.pdf)\n",
        "+ Using triplet loss for face recognition: [Schroff et al: FaceNet: A Unified Embedding for Face Recognition and Clustering (2015)](https://arxiv.org/pdf/1503.03832.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFFn_-TnSJbo"
      },
      "source": [
        "### New \"softmax variants\"\n",
        "\n",
        "The training difficulties associated with triplet loss led to the development of new, similarity metric-oriented variants of the traditional softmax--cross entropy objective for classification. Recall that for one-hot encoded classification training data, softmax-cross entropy boils down to a negative log-likelihood objective:\n",
        "\n",
        "$$\\mathcal L_\\mathrm{Softmax} = - \\log(P(y)) $$\n",
        "\n",
        "where thinking in terms of the last fully connected layer of a classification network we have further\n",
        "\n",
        "$$-\\log(P(y)) = -\\log\\left(\\frac{\\exp(\\mathbf w_y \\mathbf x + b_y)}{\\sum_c \\exp(\\mathbf w_c \\mathbf x + b_c) }\\right)$$\n",
        "\n",
        "where $\\mathbf x$ is the embedding and $y$ the correct class of the example in question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYtJfCZBSJbo"
      },
      "source": [
        "#### Angular softmax (A-softmax, 2018)\n",
        "\n",
        "Angular softmax is based on the recognition that with normalized weight vectors and zero biases (this if of course an important modification in itself!!) the softmax loss can actually be rewritten as\n",
        "\n",
        "$$-\\log\\left(\\frac{\\exp(\\mathbf w_y \\mathbf x)}{\\sum_c \\exp(\\mathbf w_c \\mathbf x) }\\right) = -\\log\\left(\\frac{\\exp(\\|\\mathbf x\\| \\cos \\theta_{\\mathbf x, \\mathbf w_y })}{\\sum_c \\exp( \\|\\mathbf x\\| \\cos \\theta_{\\mathbf x, \\mathbf w_c })}\\right) = -\\log\\left(\\frac{\\exp(\\cos \\theta_{\\mathbf x, \\mathbf w_y })}{\\sum_c \\exp( \\cos \\theta_{\\mathbf x, \\mathbf w_c })}\\right)$$\n",
        "\n",
        "that is, this modified version requires the angle between the embedding and the correct class's weight vector to be smaller then the angles to those of the other classes. The main idea of angular softmax is to strengthen this requirement to the criterion that $m$ times the angle should still be significantly smaller (introducing an \"angular margin\"), where $m\\geq 2$ is a hyperparameter, so we have\n",
        "\n",
        "$$\n",
        "-\\log\\left(\\frac{\\exp(\\cos (m  \\theta_{\\mathbf x, \\mathbf w_y }))}{\\exp(\\cos (m \\theta_{\\mathbf x, \\mathbf w_y })) + \\sum_{c\\neq y}\\exp( \\cos \\theta_{\\mathbf x, \\mathbf w_c })}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d0EIZ8aSJbo"
      },
      "source": [
        "A slight problem is caused by the fact that the cosine function is not monotonic outside the $[0, \\pi]$ interval, so $\\theta_{\\mathbf x, \\mathbf w_y}$ here must be restricted to the $[0, \\frac{\\pi}{m}]$ interval. The problem can be solved by using a cosine derivative which is monotonic in the full $[0, \\pi]$, concretely\n",
        "\n",
        "$$\\phi(\\theta) = (-1)^k \\cos(m \\theta) - 2k$$\n",
        "$$\\left(\\theta\\in \\left[\\frac{k\\pi}{m},\\frac{(k+1)\\pi}{m}\\right], k \\in [0, m-1]\\right).$$ \n",
        "\n",
        "With this change we have the final form which is\n",
        "\n",
        "$$\n",
        "\\mathcal L_{\\mathrm{A-softmax}} = -\\log\\left(\\frac{\\exp(\\phi (\\theta_{\\mathbf x, \\mathbf w_y }))}{\\exp(\\phi ( \\theta_{\\mathbf x, \\mathbf w_y })) + \\sum_{c\\neq y}\\exp( \\cos \\theta_{\\mathbf x, \\mathbf w_c })}\\right).\n",
        "$$\n",
        "For further details see the paper in which A-Softmax was introduced: [Huang et al: Angular Softmax for Short-Duration Text-independent Speaker Verification (2018)](https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1545.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbneNWuVSJbo"
      },
      "source": [
        "#### Additive margin softmax  (AM-softmax, 2018)\n",
        "\n",
        "Another softmax variant built on the same basic idea is the so called additive margin softmax loss, which demands that the embedding should be $m$ more similar to the correct class's weight vector than to the others:\n",
        "\n",
        "$$\n",
        "\\mathcal L_{\\mathrm{AM-softmax}} = -\\log\\left(\\frac{\\exp(s\\cdot\\cos (\\theta_{\\mathbf x, \\mathbf w_y }-m))}{\\exp(s\\cdot \\cos ( \\theta_{\\mathbf x, \\mathbf w_y }-m)) + \\sum_{c\\neq y}\\exp(s\\cdot\\cos \\theta_{\\mathbf x, \\mathbf w_c })}\\right)\n",
        "$$\n",
        "\n",
        "where $s$ is a scaling hyperparameter. \n",
        "\n",
        "See the original paper for further details: [Wang et al: Additive Margin Softmax for Face Verification (2018)](https://arxiv.org/pdf/1801.05599.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snp6xIEYSJbo"
      },
      "source": [
        "## Autoencoders and GAN-s\n",
        "\n",
        "An obvious follow up of the question raised by representation learning is, whether we can use unsupervised learning techniques to learn good representations of data.\n",
        "\n",
        "This is all the more important, since in most cases we have **exponentially more raw data then labeled data**, so if we could pre-train our models on a broad raw dataset with unsupervised techniques, we could learn a lot about the world.\n",
        "\n",
        "In fact some scholars, notably [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) argues that enabling broad scale unsupervised learning is the key to general intelligence.\n",
        "\n",
        "<a href=\"https://i2.wp.com/syncedreview.com/wp-content/uploads/2019/02/image-1a.png?resize=784%2C502&ssl=1\"><img src=\"https://drive.google.com/uc?export=view&id=16PSrj3ujKoA-93zMKYnLMEf0p64vy8cE\" width=70%></a>\n",
        "\n",
        "(There are also deep connections between un/self supervised learning and theories of mind, see eg. the theory of [predictive coding](https://en.wikipedia.org/wiki/Predictive_coding).)\n",
        "\n",
        "\n",
        "### [\"The Ganfather\"](https://www.technologyreview.com/s/610253/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/)\n",
        "\n",
        "In the field of unsupervised learning [Ian Goodfellow](https://en.wikipedia.org/wiki/Ian_Goodfellow) made great contributions with the elaboration of the GAN architecture. LeCun attributes him with the start of the \"Generative Revolution\" inside the DL field.\n",
        "\n",
        "<a href=\"https://www.deeplearningitalia.com/wp-content/uploads/2018/03/56180123458e517763fae26da757a924.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1DGE58IvyDD8GvXPg13acRF56Y9F_ac3P\" width=400 heigth=400></a>\n",
        "\n",
        "His in-depth [\"Deep Learning Book\"](https://www.deeplearningbook.org/) became somewhat of a canonical work, definitely worth reading.\n",
        "\n",
        "### Architecture of AEs and GANs\n",
        "\n",
        "The first widespread unsupervised neural models were the so called autoencoders.\n",
        "\n",
        "Autoencoders are unsupervised, or more properly **self-supervised learning models** that are trained to reconstruct the original data (with some noise or as sampling from a data distribution). \n",
        "\n",
        "\"According to the history provided in Schmidhuber, [\"Deep learning in neural networks: an overview,\", Neural Networks (2015)](https://arxiv.org/abs/1404.7828), auto-encoders were proposed as a method for unsupervised pre-training in Ballard, \"Modular learning in neural networks,\" Proceedings AAAI (1987). It's not clear if that's the first time auto-encoders were used, however; it's just the first time that they were used for the purpose of pre-training ANNs.\" ([soruce](https://stats.stackexchange.com/questions/238381/what-is-the-origin-of-the-autoencoder-neural-networks))\n",
        "\n",
        "Nowdays the purpose of this exercise is not pre-training (since \"depth\" is more or less conquered), but to learn dense \"semantic\" representations of the data.\n",
        "\n",
        "The big \"trick\" in autoencoders is the usage of the right objective and learning setting, since in the [words of Francois Chollet](https://blog.keras.io/building-autoencoders-in-keras.html):\n",
        "\n",
        "\"In order to get self-supervised models to learn interesting features, you have to come up with an interesting synthetic target and loss function, and that's where problems arise: merely learning to reconstruct your input in minute detail might not be the right choice here. At this point there is significant evidence that focusing on the reconstruction of a picture at the pixel level, for instance, is not conductive to learning interesting, abstract features of the kind that label-supervized learning induces (where targets are fairly abstract concepts \"invented\" by humans such as \"dog\", \"car\"...). In fact, one may argue that the best features in this regard are those that are the worst at exact input reconstruction while achieving high performance on the main task that you are interested in (classification, localization, etc).\"\n",
        "\n",
        "Because of the limitations of Autoencoders, [Goodfellow et al.](https://arxiv.org/abs/1406.2661) came up with the idea of a \"Generative adversarial network\" (GAN) training regime, whereby a generative (forger) network is trained jointly with a \"discriminator\" network, which provides the (inverse) gradients.\n",
        "\n",
        "Let's discuss these models in detail!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-17T14:25:49.591891Z",
          "start_time": "2019-09-17T14:25:49.557284Z"
        },
        "hideCode": true,
        "hidePrompt": true,
        "id": "W3zL5-DUSJbp",
        "outputId": "c949ccbc-04f0-4791-d314-cf975887baca"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vTeO6wBbmDp4pCyEqd9VPRIdqZ_nV__cPbr83ofA41mtnR5MZXMaQf1-NBnfKpYcxJqcgnHdsSoll0G/embed?start=false&loop=true&delayms=60000\" frameborder=\"0\" width=\"600\" height=\"600\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vTeO6wBbmDp4pCyEqd9VPRIdqZ_nV__cPbr83ofA41mtnR5MZXMaQf1-NBnfKpYcxJqcgnHdsSoll0G/embed?start=false&loop=true&delayms=60000\" frameborder=\"0\" width=\"600\" height=\"600\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UGmX3hmSJbq"
      },
      "source": [
        "\n",
        "GANs are representing an amazing state of the art in creating novel content, but their main appeal is **unsupervised learning**, with the potential to learn joint and general representation of phenomena, which many - most notably [Yann LeCun from Fcebook AI Research](http://yann.lecun.com/) regard as the major step towards general artificail intelligence.\n",
        "\n",
        "<a href=\"https://cdn-images-1.medium.com/max/1600/1*KDvA9Fq3lm-eQOyGlcKAKg.png\"><img src=\"https://drive.google.com/uc?export=view&id=1qa_ytGej9ZH_6dSW_TD-ssIG6xRzIhcd\" width=600 heigth=600></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yXm7zLwSJbq"
      },
      "source": [
        "### What can they be good for?\n",
        "\n",
        "- GAN-s are also capable of acting on video streams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-17T14:25:55.495004Z",
          "start_time": "2019-09-17T14:25:55.479663Z"
        },
        "hideCode": true,
        "hidePrompt": true,
        "id": "UFyUPN0OSJbq",
        "outputId": "f130a67f-f76d-450b-effd-e087de83ce16"
      },
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Nq2xvsVojVo\" frameborder=\"0\" width=\"600\" heigth=\"600\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Nq2xvsVojVo\" frameborder=\"0\" width=\"600\" heigth=\"600\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UEWPch-SJbr"
      },
      "source": [
        "- It can handle acoustic inputs also, see for example these [voice cloning experiments](https://audiodemos.github.io/)\n",
        "- It can enhance creativity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-17T14:25:59.288298Z",
          "start_time": "2019-09-17T14:25:59.275897Z"
        },
        "hideCode": true,
        "hidePrompt": true,
        "id": "vCGUAWodSJbr",
        "outputId": "a6fa27fa-b07d-4fbc-9138-d756ec9f11af"
      },
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hW1_Sidq3m8\" frameborder=\"0\" width=\"600\" heigth=\"600\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hW1_Sidq3m8\" frameborder=\"0\" width=\"600\" heigth=\"600\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o41O4EOsSJbr"
      },
      "source": [
        "**More generally:**\n",
        "\n",
        "- New instance generation (hopefully in a controlled manner)\n",
        "- Input for longer (classifier) pipelines\n",
        "- Similarity search, clustering\n",
        "...and many more things that is only bounded by creativity. :-)\n",
        "\n",
        "### Play with GANs\n",
        "\n",
        "There is a very nice recent visualization tool, [Play with Generated Adversarial Networks (GANs) in your browser!\n",
        "](https://poloclub.github.io/ganlab/)\n",
        "\n",
        "Since the dynamics of GAN training is non  trivial, it is worth studying.\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "The generative paradigm shift is considered one of the frontiers of AI (together with reinforcement learning, \"zero shot\" and \"multi task\" learning - to name a few). It is well worth watching this space!\n",
        "\n",
        "### Caveat: We are not yet there!\n",
        "\n",
        "<a href=\"https://1.bp.blogspot.com/-KwCuE2PZccs/XL-XNmDenoI/AAAAAAAAEF8/rMwS1PepVk40nuX0TvcK52d9NBv6IBziwCLcBGAs/s640/ground-truth-imagemagick%252Bcoalesce.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1cvid_ggQVNFVkn4kzahNdx7qn4ynfOAf\" width=55%></a>\n",
        "\n",
        "Unsupervised learning of the **real causal factors** and a nice, independent and interpretable (\"disentangled\") representation of them would be the \"holy grail\" of machine learning.\n",
        "\n",
        "Unfortunately, Google AI researchers have conducted large scale experiments with such models, and the results are somewhat disheartening.\n",
        "\n",
        "\"We propose a fair, reproducible experimental protocol to benchmark the state of unsupervised disentanglement learning by implementing **six different state-of-the-art models** (BetaVAE, AnnealedVAE, FactorVAE, DIP-VAE I/II and Beta-TCVAE) and **six disentanglement metrics** (BetaVAE score, FactorVAE score, MIG, SAP, Modularity and DCI Disentanglement). **In total**, we train and evaluate **12,800 such models on seven data sets**. \n",
        "\n",
        "Key findings of our study include:\n",
        "\n",
        "- We **do not find any empirical evidence that the considered models can be used to reliably learn disentangled representations in an unsupervised way**, since **random seeds** and hyperparameters seem to **matter more than the model choice.** In other words, even if one trains a large number of models and some of them are disentangled, these disentangled representations seemingly cannot be identified without access to ground-truth labels. Furthermore, good hyperparameter values do not appear to consistently transfer across the data sets in our study. These results are consistent with the theorem we present in the paper, which states that the unsupervised learning of disentangled representations is impossible without inductive biases on both the data set and the models (i.e., one has to make assumptions about the data set and incorporate those assumptions into the model).\n",
        "- For the considered models and data sets, **we cannot validate the assumption that disentanglement is useful for downstream tasks**, e.g., that with disentangled representations it is possible to learn with fewer labeled observations.\"\n",
        "\n",
        "\n",
        "More detailed results [here](https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html).\n",
        "\n",
        "For now, **we have no choice but use the _right_ inductive bias.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBuYH0aySJbt"
      },
      "source": [
        "<a id=\"rl\"></a>\n",
        "# Reinforcement learning - as outlook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1dSoQkdSJbt"
      },
      "source": [
        "- Most widely applicable learning paradigm in AI\n",
        "- Adds a lot to it's appeal, but it also is the cause of it's drawbacks: extreme hunger for computation, brittle and slow convergence of learning. But when it succeeds, it suceeds big time! :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRQJKjeNSJbt"
      },
      "source": [
        "## Short history, main milestones"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the 50s Richard Bellman investigated problems of \"optimal control\", which were aiming at optimizing the behavior of physical systems. He proposed the **Bellman equations**, the foundational concepts of dynamic programming, as well as the **Markov Decision Process**, which is the discrete stochastic case of optimal control (see below). He did not mention learning at all.\n",
        "\n",
        "In parallel to this in psychology the **behaviorist paradigm** rose to dominance, which interpreted the learning of animal in frames pf a **\"trial and error\"** process. It took the two fields, optimal control and behaviorism couple of decades to meet. In 1989 Chris Watkins formalized the reinforcement paradigm of learning as Markov Decision Process. This idea saw widespread adoption.\n",
        "\n",
        "Later Dimitri Bertsekas and John Tsitsiklis started to experiment with combining dynamic programming and neural networks (1996).\n",
        "\n",
        "Around 1983 - 1986 Sutton, Barto, Anderson realized breakthroughs with the so called  temporal-difference learning, which is an important part of today's RL algorithms. This method formed the basis of the TD-gammon algorithm of Gerald Tesauro, which was successful in Backgammon. This was a comparable breakthrough in 1992 to the one for Go in 2015."
      ],
      "metadata": {
        "id": "yLeLSOeUvfbY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWd4-vmaSJbt"
      },
      "source": [
        "\n",
        "#### Recent success\n",
        "\n",
        "In 2013 the team of Valdimir Mnih taught a neural network to play Atari games. During the teaching the agent only received the pixel representation of the screen as input, no information about game rules were give. One of the classic games on the platform was Breakout, which the DQN algorithm of DeepMind mastered perfectly. [Video](https://www.youtube.com/watch?v=TmPfTpjtdgg)\n",
        "\n",
        "An even bigger breakthrough happened in 2015, when the AlphaGo model of Google DeepMind was able to beat a professional player (Fan Hui) for the first time on a 19x19  field. Following this it was able to overcome the 9 dan player Li Sedol  in 2016 by 4:1, and finally in 2017 Ke Jie, the supposedly best player in the world by 3:0. (In some opinions this was the \"sputnik moment\" of Chinese consciousness and fastened the elaboration of the Chinese state's massive AI iniciative.) The next step was the development of AlphaZero which is able to dominantly beat AlphaGo. The main advantage of the new model is, that it does not contain any hand crafted features which would aid it in recognizing game patterns. It is a truely end-to-end system. Go as a testbed is all the more remarkable, since it has a very high branching faktor, that the possibilities in it's state space explode rapidly. [Summary video](https://www.youtube.com/watch?v=SUbqykXVx0A<a href=\"http://zone.msn.com/images/v9/en-us/game/bckg/380x285_bckg.gif\">https://drive.google.com/uc?export=view&id=1b_zpsn5-nAvfS32z8Bh5MWJLeb3XoWF4) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxYnNlxUSJbu"
      },
      "source": [
        "## Basic RL "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqeiL_x1SJbu"
      },
      "source": [
        "![rlmodel](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ8ozFJfJXjRQj6vKNaKYu9wi7IKb6YFt5ilCwv00XXGjD0rmL-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_pwRoeqSJbu"
      },
      "source": [
        "The most important concepts in RL are:\n",
        "* state (and observation)\n",
        "* action\n",
        "* immediate reward, return\n",
        "* policy\n",
        "* environment (its dynamics).\n",
        "\n",
        "We try to illustrate these with the extended example of chess play.\n",
        "![chesstable](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/ChessSet.jpg/250px-ChessSet.jpg)\n",
        "\n",
        "**State:** Totality of parameters describing the environment. \n",
        "- In chess this is the position of the chess pieces. \n",
        "- If we remove a piece and all else remains the same, it is considered a new state<br/>\n",
        "\n",
        "**Observation:** how an agent perceives this state (the representation of the state). \n",
        "- If it cannot measure everything exactly or has no full access to state information, it can identify potentially different states as the same in lack of information. \n",
        "- In chess when we see the table then state == observation, but in the chess variant Kriegsspiel we do not. \n",
        "- Observation is in most of the cases lacking some information about state.\n",
        "\n",
        "**Action:** mode of modifying the system by taking action and thus causing state change in the system. \n",
        "- In chess this is taking a move with a piece. \n",
        "- The space of possible actions is dependent also on the state and its representation!\n",
        "\n",
        "**Immediate reward, return:** \n",
        "- After taking action the agent can potentially receive feedback, what we call _immediate reward_. \n",
        "- The more interesting element though, is the so called _return_ (or utility), which is an accumulated form of immediate rewards during long-running action sequence of the agent. \n",
        "- Return can not be any kind of function, it has to satisfy the requirement of stationarity illustrated below:\n",
        "\n",
        "<a href=\"http://drive.google.com/uc?export=view&id=1Iji_ykwG0WEVSmBkpwwZe8TqgTT5VJad\"><img src=\"https://drive.google.com/uc?export=view&id=19KM7Q-I7WW0F38xOOTlMtx9eUV4u02A1\" width=\"700px\"></a>\n",
        "\n",
        "If it seems like the red trajectory is more rewarding when we decide in $s_0$, then the utility of the green trajectory must be higher than for the brown one. This can be satisfied by two types of returns (sum, discounted):\n",
        "\n",
        "$$ G(\\tau) = \\sum_{r_i \\in \\tau} {r_i},$$\n",
        "\n",
        "and:\n",
        "\n",
        "$$ G(\\tau) = \\sum_{r_i \\in \\tau} {\\gamma^i r_i}.$$\n",
        "\n",
        "Where $0 < \\gamma < 1$, $\\tau$ is the trajectory, that is a sequence ($s_0, a_0, r_0, s_1, a_1, r_1, \\dots s_i, a_i, r_i \\dots$), $r_i$ is the next element in the trjectory. In our chess example, there is no immediate reward, only at the end one reward signal for winning or loosing. We could try to calculate points even during the gameplay, but that would be misleading, since it would not define properly if we will win or loose.\n",
        "\n",
        "**Policy:** The agent is forced to make a decision in every state. InRL, we suppose, that time is discretized, that there are \"steps\". Put simply _policy_ is a function assigning an action to a state $\\pi: S \\rightarrow A$, or more precisely: \n",
        "\n",
        "$$\\pi(s) = p(a|s).$$\n",
        "\n",
        "This formula means that policy is a function, that assigns a probability distribution over the possible actions for each state, that is it shows how much it is \"worth\" to choose a given action in a given state (in the form of a probability value). This type of policy we call a \"stochastic policy\". If the policy is structured as assigning the probability of $1$ to only one action, while all others are $0$, we talk about a deterministic policy.\n",
        "\n",
        "**Environment:** The agent acts in an environment, which transfers to a different state based on every occurence. This is defined by the dynamics of the environment. In chess we move AND the opponent moves also.\n",
        "\n",
        "Technically the following are possible:\n",
        "* Environment can be stochastic or deterministic\n",
        "* Environment can be partially or fully observable (I see the complete state in this case)\n",
        "* Action can be discrete or continuous (move a piece vs. turn apply more pressure on the accelerator pedal)\n",
        "* Policy can be deterministic or stochastic\n",
        "\n",
        "\n",
        "**The goal function of RL is the return itself. We try to maximize this.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbebR9I0SJbu"
      },
      "source": [
        "## Deep reinforcement learning\n",
        "\n",
        "**In one sentence: We use deep neural networks to approximate policy and/or value functions and utilize gradient based or evolutionary learning methods for their teachnig.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where is this relevant?\n",
        "\n",
        "Reinforcement learning is capable of attacking problems and domains, where:\n",
        "- Data is unlabeled and potentially unlabelable. It is simply infeasible to label each and every second of a road driving session with a label of \"good move - bad move\" based on the driver behavior.\n",
        "- Complex optimization scenarios, where we can tell something about the \"better or worse\" situations, but can ónot explicitly come up with strategy elements that have to be evaluated.\n",
        "- **It is an end-to-end learning approach taken to the extreme, meaning: we only know the \"reward\" but nothing about input processing, features, policy elements, let alone the final policy.**\n",
        "\n",
        "\n",
        "One of the main areas of application for RL is robotics and process control.\n",
        "\n",
        "The Japanese company [Fanuc](https://www.fanuc.com/)  build robots that can learn to carry out a new task within a day. (Mainly moving things aroud.)\n",
        "\n",
        "It is also an eminent application, when we try to optimize the energy consumption of complex machinery or production processes (Google DeepMind uses it for cooling control of server farms).\n"
      ],
      "metadata": {
        "id": "yOscpm5bxgSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### The most suitable problems for RL:\n",
        "The areas most suited for RL are:\n",
        "* control problems (though Sutton's results allow us to generalize to prediction - not a common technique)\n",
        "* the system is described with a huge amount of complex parameters, feature engineering is hard\n",
        "* the intervention signal for control is very complex\n"
      ],
      "metadata": {
        "id": "JW842DfYxp4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pros and cons of RL\n",
        "**Pro:**\n",
        "- Extremely promising as a general AI learning framework (plausibly we humans do reinforcement learning)\n",
        "- Can enter domains where nothing else can proceed\n",
        "- _Huge amount_ of innovation is going on (just like with GANs)\n",
        "\n",
        "**Cons:**\n",
        "- Still a distinct subfield of AI (though interest is catching up, but it is still a distinct \"tribe\")\n",
        "- Training is notoriously **instable** and exceptionally **computation hungry** (weeks on GPU clusters)\n",
        "- It is still quite poorly understood.\n",
        "- **It depends on the ability to try out billions of actions, whoch is only feasible in a good simulator** (Simulators are themselves costly, there is a transfer learning problem between the simulator and the real environment... Interesting idea is to use neural models as simulators, this is pointiong towards a GAN style approach, see [World models - can agents learn from their dreams?](https://worldmodels.github.io/))\n",
        "\n",
        "For a detailed description of the problems see [here](https://www.alexirpan.com/2018/02/14/rl-hard.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "4ufdUwA4xs8T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdfAkG06SJbv"
      },
      "source": [
        "### Further reading\n",
        "\n",
        "#### Books\n",
        "\n",
        "* Sutton and Barto, Reinforcement Learning: An introduction, 2018, [Link](http://incompleteideas.net/book/RLbook2020.pdf) \n",
        "* Szepesvári Csaba, Algorithms for Reinforcement Learning, 2009, [Link](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs-lecture.pdf)\n",
        "\n",
        "#### Blogs, websites\n",
        "\n",
        "* Andrej Karpathy, Deep Reinforcement Learning: Pong from Pixels [Link](http://karpathy.github.io/2016/05/31/rl/)\n",
        "* DeepMind, Deep Reinforcement Learning, [Link](https://deepmind.com/blog/deep-reinforcement-learning/)\n",
        "* David Silver, Reinforcement Learning Courses, [Link](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html)\n",
        "\n",
        "#### Articles\n",
        "\n",
        "* Mnih et. al,Human-level control through deep reinforcement learning, 2015, [Link](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)\n",
        "\n",
        "* Silver et. al, Mastering the game of Go without human knowledge, 2017, [Link](https://www.nature.com/articles/nature24270.pdf)\n",
        "\n",
        "* Mnih et. al, Asynchronous Methods for Deep Reinforcement Learning, 2016, [Link](https://arxiv.org/pdf/1602.01783.pdf)"
      ]
    }
  ]
}