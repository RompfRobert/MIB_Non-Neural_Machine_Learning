{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Using keras"
      ],
      "metadata": {
        "id": "qsO8KKzZbg3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G4IByu_bfNJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "From https://archive.ics.uci.edu/ml/datasets/heart+Disease\n",
        "\n",
        "> **target**: diagnosis of heart disease (angiographic disease status)\n",
        "-- Value 0: < 50% diameter narrowing\n",
        "-- Value 1: > 50% diameter narrowing\n",
        "\n",
        "> **sex**: sex (1 = male; 0 = female)\n",
        "\n",
        "> **cp**: chest pain type\n",
        "-- Value 1: typical angina\n",
        "-- Value 2: atypical angina\n",
        "-- Value 3: non-anginal pain\n",
        "-- Value 4: asymptomatic"
      ],
      "metadata": {
        "id": "a9Fr9dVmboWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
        "\n",
        "## read the csv file under the file_url into a pandas dataframe\n",
        "df = ..."
      ],
      "metadata": {
        "id": "tVzF5F_xbgY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "g2PYsuIMbmNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct4iANnmfMsB",
        "outputId": "5cc89f69-60a7-403d-cf8b-982a14186603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 303 entries, 0 to 302\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       303 non-null    int64  \n",
            " 1   sex       303 non-null    int64  \n",
            " 2   cp        303 non-null    int64  \n",
            " 3   trestbps  303 non-null    int64  \n",
            " 4   chol      303 non-null    int64  \n",
            " 5   fbs       303 non-null    int64  \n",
            " 6   restecg   303 non-null    int64  \n",
            " 7   thalach   303 non-null    int64  \n",
            " 8   exang     303 non-null    int64  \n",
            " 9   oldpeak   303 non-null    float64\n",
            " 10  slope     303 non-null    int64  \n",
            " 11  ca        303 non-null    int64  \n",
            " 12  thal      303 non-null    object \n",
            " 13  target    303 non-null    int64  \n",
            "dtypes: float64(1), int64(12), object(1)\n",
            "memory usage: 33.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create a new dataframe where the [\"sex\", \"cp\", \"thal\"] columns are one-hot encoded.\n",
        "\n",
        "df_new = ...\n"
      ],
      "metadata": {
        "id": "ssKX-EnMgLhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-valid-test split"
      ],
      "metadata": {
        "id": "-nOV_wS40YAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## import scikit-learn's StandardScaler and train_test_split\n",
        "..."
      ],
      "metadata": {
        "id": "JawPXkuU0af9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create the input array X and target array y from df_new.\n",
        "## y should be the 1-dimensional numpy array containing the values of the \"target\" column,\n",
        "## X should be the 2-dimensional numpy array containing the values of the df_new EXCEPT for hte \"target\" column.\n",
        "\n",
        "targetcol = \"target\"\n",
        "\n",
        "X = ...\n",
        "y = ...\n",
        "\n",
        "## print out the shapes of X and y to check they're compatible.\n",
        "...."
      ],
      "metadata": {
        "id": "JSIc-ms9hw_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Do a shuffled train-test-validation split. \n",
        "## Train data (X_train, y_train) should be 70% of the data, \n",
        "## test (X_test, y_test) and validation (X_val, y_val) 15-15% each.\n",
        "## Use random seeding with seed 42 to obtain reproducible results!\n",
        "\n",
        "...\n",
        "\n",
        "print(\"\", X_train.shape, \"\\n\", y_train.shape, \n",
        "      \"\\n\", X_val.shape, \"\\n\", y_val.shape,\n",
        "      \"\\n\", X_test.shape, \"\\n\", y_test.shape)"
      ],
      "metadata": {
        "id": "wdqozoC-wCiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: create scaled data"
      ],
      "metadata": {
        "id": "GJB2yBHGCBEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create a standardscaler, fit it on X_train and create a transformed X_..._scaled scaled array from all X splits.\n",
        "..."
      ],
      "metadata": {
        "id": "a4Jc_SNI0jhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\", X_train_scaled.shape, \"\\n\", y_train.shape, \n",
        "      \"\\n\", X_val_scaled.shape, \"\\n\", y_val.shape,\n",
        "      \"\\n\", X_test_scaled.shape, \"\\n\", y_test.shape)"
      ],
      "metadata": {
        "id": "oLwGGraC1Myj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## a little extra: visualization after decomposition\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## dimred with PCA into 2D\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(2)\n",
        "X_transformed = pca.fit_transform(X_scaled)\n",
        "sns.scatterplot(data=pd.DataFrame(X_transformed, columns=[\"PC1\", \"PC2\"]),\n",
        "                x=\"PC1\", y=\"PC2\", hue=y);\n",
        "plt.title(\"PCA dim. red.\")\n",
        "plt.show()\n",
        "\n",
        "## dimred with PCA into 1D\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "X_transformed = lda.fit_transform(X=X_scaled, y=y)\n",
        "sns.scatterplot(x=X_transformed[:,0], y=y, hue=y)\n",
        "plt.xlabel(\"new_dim\")\n",
        "plt.ylabel(\"target\")\n",
        "plt.title(\"LDA dim. red.\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zv0-DL0-qw46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keras model using sequential API"
      ],
      "metadata": {
        "id": "6w72auJijisW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## import the Dense layer and Sequential class from tf.keras.\n",
        "## import tensorflow with its usual abbreviation.\n",
        "...\n",
        "...\n",
        "..."
      ],
      "metadata": {
        "id": "a0gbU8Ddkbtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Write the function which creates a keras model using the sequential API.\n",
        "## The model should have 1 layer, which should have 3 neurons in it, and use the 'sigmoid' activation function.\n",
        "## (For this first layer, feel free to play around with other activation functions, or even lack of it, \n",
        "## which is None, that is simple linear activation.)\n",
        "## The final (output) layer should have 1 unit, and the 'sigmoid' activation function.\n",
        "## Then compile the model, using \"binary_crossentropy\" loss, \"Adam\" optimizer, and [\"accuracy\"] metrics\n",
        "\n",
        "def create_model():\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    ## add the first layer \n",
        "    ...\n",
        "\n",
        "    ## add the output layer\n",
        "    ...\n",
        "\n",
        "    ## compile the model\n",
        "    ...\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "2sBU2TE31bkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## now write a function which creates a model, prints out a summary, fits it, and plots the history.\n",
        "\n",
        "def create_and_fit_model(X_train, y_train, X_val, y_val, num_epochs, num_batchsize):\n",
        "    tf.keras.utils.set_random_seed(42)\n",
        "    model = create_model()\n",
        "    \n",
        "    ## print out model summary\n",
        "    print(...)\n",
        "\n",
        "    ## Fit the model and store training history in the history variable.\n",
        "    ## Fit it on the X_train and y_train data.\n",
        "    ## Use num_epochs number of epochs, num_batchsize as batch size.\n",
        "    history = model.fit(...,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        )\n",
        "                        \n",
        "    ## Plotting the history is done for you:\n",
        "    historydf = pd.DataFrame(history.history)\n",
        "    historydf.plot(xlabel=\"epoch\", secondary_y=[c for c in historydf.columns if c.endswith(\"loss\")]);\n",
        "\n",
        "    return model, historydf"
      ],
      "metadata": {
        "id": "3QpOr3WH6oEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model fit parameters"
      ],
      "metadata": {
        "id": "t_jNXdtm2Hj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "num_batchsize = 32"
      ],
      "metadata": {
        "id": "cmjTvr-i2JIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do fitting"
      ],
      "metadata": {
        "id": "ItsNsjdy7s5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, historydf = create_and_fit_model(X_train, y_train, X_val, y_val, num_epochs, num_batchsize)"
      ],
      "metadata": {
        "id": "sKuze-9x7Sqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Create a model and fit on scaled data"
      ],
      "metadata": {
        "id": "IfBu01Zf1-en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_scaled, historydf_scaled = create_and_fit_model(X_train_scaled, y_train, X_val_scaled, y_val, num_epochs, num_batchsize)"
      ],
      "metadata": {
        "id": "mtR1h4Ou1tio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## No task, just compare the plots if you fit the model on scaled data, too:\n",
        "allhistory = historydf.merge(historydf_scaled,right_index=True, left_index=True, suffixes=[\"\", \"_scaled\"])\n",
        "allhistory[[c for c in allhistory.columns if \"accuracy\" in c]].plot();\n",
        "allhistory[[c for c in allhistory.columns if \"loss\" in c]].plot(ylim=(0,1));"
      ],
      "metadata": {
        "id": "lz9ZLp4b7_FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Same model with functional API"
      ],
      "metadata": {
        "id": "37zJKKlz6a6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## import Input, Model from keras"
      ],
      "metadata": {
        "id": "y0LCApVN8pB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    ## create an input layer (necessary when using the functional API)\n",
        "    input_layer = ...\n",
        "\n",
        "    ## create the first layer as before, but with functional API\n",
        "    layer1 = ...\n",
        "\n",
        "    ## create the output layer as before, but with functional API\n",
        "    output_layer = ...\n",
        "\n",
        "    ## create the model that takes input_layer as input and output_layer as output.\n",
        "    model = ...\n",
        "\n",
        "    ## compile the model as before\n",
        "    ...\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hd-2j1tO6k2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## just run it again to check it works\n",
        "model, historydf = create_and_fit_model(X_train, y_train, X_val, y_val, num_epochs, num_batchsize)"
      ],
      "metadata": {
        "id": "AiFwqfBI9W6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on test set"
      ],
      "metadata": {
        "id": "M12HQl8m-ZCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## optional: evaluate model on test set using its evaluate method\n",
        "(model_loss, model_accuracy) = ...\n"
      ],
      "metadata": {
        "id": "kkmcW30u_IaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## optional: evaluate model_scaled on test set using its evaluate method\n",
        "(model_scaled_loss, model_scaled_accuracy) = model_scaled.evaluate(X_test_scaled, y_test)\n"
      ],
      "metadata": {
        "id": "lq-FIxBz_Lgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## get the predictions of the model for the test set\n",
        "y_test_pred = ...\n",
        "\n",
        "## optional: get the predictions of model_scaled for the test set\n",
        "#y_test_pred_scaled = ..."
      ],
      "metadata": {
        "id": "Xv5t2CoE9ak2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## just run the code to get classes instead of floats between 0 and 1:\n",
        "\n",
        "y_test_pred_cl = y_test_pred.round().flatten().astype(int)\n",
        "#y_test_pred_cl_scaled = y_test_pred_scaled.round().flatten().astype(int)"
      ],
      "metadata": {
        "id": "xmrwfxlu-o3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "X67Bzs9o_YZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Print out the classification report for the test set predictions.\n",
        "print(\"Non-scaled:\")\n",
        "print(...)\n",
        "\n",
        "## Optional: do the same for the predictions of model_scaled.\n",
        "#print(\"\\nScaled:\")\n",
        "#print(...)"
      ],
      "metadata": {
        "id": "P7QO207l_chG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}